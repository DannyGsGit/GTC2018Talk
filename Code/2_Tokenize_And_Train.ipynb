{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, csv, re, itertools, collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import jellyfish as jyfs\n",
    "import datetime, time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import inflect\n",
    "from random import *\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize and Train Notebook\n",
    "This notebook will take a text corpus and use it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "with open('./tempfiles/cleanParts.pkl', 'rb') as f:\n",
    "   words = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['cleaner band',\n",
       " 'odx al tube',\n",
       " 'aba al tube od',\n",
       " 'odx al tube',\n",
       " 'od al tube',\n",
       " 'od al tube',\n",
       " 'od al tube',\n",
       " 'wheel safety svce manual',\n",
       " 'whl serv manual spanish',\n",
       " 'acs aluminum coil stock all']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the dataset\n",
    "print(type(words))\n",
    "words[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Training Batches\n",
    "This function will be called during training to generate minibatches for the skip-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 4347\n"
     ]
    }
   ],
   "source": [
    "# Map words to indices\n",
    "word2index_map = {}\n",
    "index = 0\n",
    "for sent in words:\n",
    "    for word in sent.lower().split():\n",
    "        if word not in word2index_map:\n",
    "            word2index_map[word] = index\n",
    "            index += 1\n",
    "index2word_map = {index: word for word, index in word2index_map.items()}\n",
    "\n",
    "vocabulary_size = len(index2word_map)\n",
    "\n",
    "print(\"Vocab size:\", vocabulary_size)\n",
    "# print(\"Word Index:\", index2word_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'cleaner', 1: 'band', 2: 'odx', 3: 'al', 4: 'tube'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the top of the dictionary\n",
    "dict(list(index2word_map.items())[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate skip-grams\n",
    "# Initialize the skip-gram pairs list\n",
    "skip_gram_pairs = []\n",
    "\n",
    "# Set the skip-gram window size\n",
    "window_size = 2\n",
    "\n",
    "for sent in words:\n",
    "    tokenized_sent = sent.split()\n",
    "    # Set the target index\n",
    "    for tgt_idx in range(0, len(tokenized_sent)):\n",
    "        # Set range for the sentence\n",
    "        max_idx = len(tokenized_sent) - 1\n",
    "\n",
    "        # Define range around target\n",
    "        lo_idx = max(tgt_idx - window_size, 0)\n",
    "        hi_idx = min(tgt_idx + window_size, max_idx) + 1\n",
    "\n",
    "        # List the indices in the skip-gram outputs (removing target index)\n",
    "        number_list = range(lo_idx, hi_idx)\n",
    "        output_matches = list(filter(lambda x: x != tgt_idx, number_list))\n",
    "\n",
    "        # Generate skip-gram pairs\n",
    "        pairs = [[word2index_map[tokenized_sent[tgt_idx]], word2index_map[tokenized_sent[out]]] for out in output_matches]\n",
    "        # print(pairs)\n",
    "\n",
    "        for p in pairs:\n",
    "            skip_gram_pairs.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1],\n",
       " [1, 0],\n",
       " [2, 3],\n",
       " [2, 4],\n",
       " [3, 2],\n",
       " [3, 4],\n",
       " [4, 2],\n",
       " [4, 3],\n",
       " [5, 3],\n",
       " [5, 4],\n",
       " [3, 5],\n",
       " [3, 4]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect some output:\n",
    "skip_gram_pairs[0:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define a function to sample batches from the skipgram pairs during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skipgram_batch(start_index, end_index):\n",
    "    instance_indices = list(range(len(skip_gram_pairs)))\n",
    "    # np.random.shuffle(instance_indices)\n",
    "    batch = instance_indices[start_index:end_index]\n",
    "    x = [skip_gram_pairs[i][0] for i in batch]\n",
    "    y = [[skip_gram_pairs[i][1]] for i in batch]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Batch:  ['cleaner', 'band', 'odx', 'odx', 'al', 'al', 'tube', 'tube']\n",
      "Y Batch:  ['band', 'cleaner', 'al', 'tube', 'odx', 'tube', 'odx', 'al']\n"
     ]
    }
   ],
   "source": [
    "# batch example\n",
    "x_batch, y_batch = get_skipgram_batch(0,8)\n",
    "\n",
    "print(\"X Batch: \", [index2word_map[word] for word in x_batch])\n",
    "print(\"Y Batch: \", [index2word_map[word[0]] for word in y_batch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ========= To Do Training ==================\n",
    "Add the following features:\n",
    "* Number of epochs to train\n",
    "## ========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_dimension = 128\n",
    "negative_samples = 64\n",
    "n_iterations = int(round(2 * len(skip_gram_pairs) / batch_size,0))\n",
    "LOG_DIR = \"logs/word2vec_cab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  9087304  skip-gram pairs\n",
      "The chosen iteration and batch size parameters will yield  2.0  epochs.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are \", len(skip_gram_pairs), \" skip-gram pairs\")\n",
    "print(\"The chosen iteration and batch size parameters will yield \",\n",
    "     round((batch_size * n_iterations)/len(skip_gram_pairs),2), \" epochs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "# This may work with GPU\n",
    "with graph.as_default(), tf.device('/cpu:0'), tf.name_scope(\"embeddings\"):\n",
    "    # Input data, labels\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    \n",
    "    # Embedding lookup table\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_dimension],\n",
    "                          -1.0, 1.0), name='embedding')\n",
    "    # This is essentialy a lookup table\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "    \n",
    "    # Create variables for the NCE loss\n",
    "    nce_weights = tf.Variable(\n",
    "            tf.truncated_normal([vocabulary_size, embedding_dimension],\n",
    "                                stddev=1.0 / math.sqrt(embedding_dimension)))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    with tf.device('/cpu:0'):\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(weights=nce_weights,\n",
    "                           biases=nce_biases,\n",
    "                           inputs=embed,\n",
    "                           labels=train_labels,\n",
    "                           num_sampled=negative_samples,\n",
    "                           num_classes=vocabulary_size))\n",
    "        tf.summary.scalar(\"NCE_loss\", loss)\n",
    "    \n",
    "    # Learning rate decay\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    learningRate = tf.train.exponential_decay(learning_rate=0.1,\n",
    "                                              global_step=global_step,\n",
    "                                              decay_steps=1000,\n",
    "                                              decay_rate=0.95,\n",
    "                                              staircase=True)\n",
    "    train_step = tf.train.GradientDescentOptimizer(learningRate).minimize(loss)\n",
    "    merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed  0  of  141989\n",
      "Loss at 0: 180.20447\n",
      "Completed  10  of  141989\n",
      "Loss at 10: 199.43880\n",
      "Completed  20  of  141989\n",
      "Loss at 20: 190.76051\n",
      "Completed  30  of  141989\n",
      "Loss at 30: 150.21616\n"
     ]
    }
   ],
   "source": [
    "max_index = len(skip_gram_pairs)\n",
    "start_index = 0\n",
    "end_index = min(start_index + batch_size, max_index)\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    train_writer = tf.summary.FileWriter(LOG_DIR,\n",
    "                                         graph=tf.get_default_graph())\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with open(os.path.join(LOG_DIR, 'metadata.tsv'), \"w\") as metadata:\n",
    "        metadata.write('Name\\tClass\\n')\n",
    "        for k, v in index2word_map.items():\n",
    "            metadata.write('%s\\t%d\\n' % (v, k))\n",
    "\n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding = config.embeddings.add()\n",
    "    embedding.tensor_name = embeddings.name\n",
    "    # Link this tensor to its metadata file (e.g. labels).\n",
    "    embedding.metadata_path = os.path.join(LOG_DIR, 'metadata.tsv')\n",
    "    projector.visualize_embeddings(train_writer, config)\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for step in range(n_iterations):\n",
    "        x_batch, y_batch = get_skipgram_batch(start_index, end_index)\n",
    "        summary, _ = sess.run([merged, train_step],\n",
    "                              feed_dict={train_inputs: x_batch,\n",
    "                                         train_labels: y_batch})\n",
    "        train_writer.add_summary(summary, step)\n",
    "        \n",
    "        \n",
    "        if start_index >= max_index:\n",
    "            start_index = 0\n",
    "        else:\n",
    "            start_index = start_index + batch_size + 1\n",
    "        \n",
    "        end_index = min(start_index + batch_size, max_index)\n",
    "        \n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print(\"Completed \", step, \" of \", n_iterations)\n",
    "            saver.save(sess, os.path.join(LOG_DIR, \"w2v_model.ckpt\"), step)\n",
    "            loss_value = sess.run(loss,\n",
    "                                  feed_dict={train_inputs: x_batch,\n",
    "                                             train_labels: y_batch})\n",
    "            print(\"Loss at %d: %.5f\" % (step, loss_value))\n",
    "\n",
    "    # Normalize embeddings before using\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    normalized_embeddings_matrix = sess.run(normalized_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
